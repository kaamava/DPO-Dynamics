#!/usr/bin/env python3
"""
AlpacaEvalä¸¤æ¨¡å‹å¯¹æ¯”è¯„ä¼°è„šæœ¬ - ä½¿ç”¨VLLMè¿›è¡Œé«˜æ•ˆæ¨ç†
"""

import os
# è®¾ç½®ç¯å¢ƒå˜é‡ï¼Œé¿å…åˆ†å¸ƒå¼å’Œç½‘ç»œé—®é¢˜
if "CUDA_VISIBLE_DEVICES" not in os.environ:
    os.environ["CUDA_VISIBLE_DEVICES"] = "0"  # åªä½¿ç”¨ç¬¬ä¸€ä¸ªGPUï¼ˆå¦‚æœæœªè®¾ç½®ï¼‰
os.environ["VLLM_USE_MODELSCOPE"] = "False"
os.environ["VLLM_WORKER_MULTIPROC_METHOD"] = "spawn"
os.environ["NCCL_P2P_DISABLE"] = "1"  # ç¦ç”¨P2Pé€šä¿¡
os.environ["NCCL_IB_DISABLE"] = "1"   # ç¦ç”¨InfiniBand

import json
import argparse
import subprocess
import sys
import torch
from pathlib import Path
from datetime import datetime
from transformers import AutoTokenizer
from datasets import load_dataset
from vllm import LLM, SamplingParams

def generate_model_outputs(checkpoint_path, model_name, max_samples=10):
    """
    ä½¿ç”¨VLLMç”Ÿæˆæ¨¡å‹è¾“å‡º
    """
    print(f"ğŸ”„ ç”Ÿæˆ{model_name}æ¨¡å‹è¾“å‡ºï¼Œæ ·æœ¬æ•°é‡: {max_samples if max_samples != -1 else 'å…¨éƒ¨'}")
    
    # åŠ è½½åˆ†è¯å™¨
    print(f"ğŸ“¥ åŠ è½½åˆ†è¯å™¨: {checkpoint_path}")
    tokenizer = AutoTokenizer.from_pretrained(checkpoint_path, trust_remote_code=True)
    
    # è®¾ç½®pad_token
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    # åˆå§‹åŒ–VLLMæ¨¡å‹
    print(f"ğŸš€ åˆå§‹åŒ–VLLMæ¨¡å‹: {checkpoint_path}")
    
    # ä½¿ç”¨ç®€å•çš„å•GPUé…ç½®ï¼Œé¿å…åˆ†å¸ƒå¼é—®é¢˜
    print(f"ğŸ”§ ä½¿ç”¨å•GPUæ¨¡å¼ï¼Œé¿å…åˆ†å¸ƒå¼è¿æ¥é—®é¢˜")
    
    llm = LLM(
        model=checkpoint_path,
        dtype="float16",
        trust_remote_code=True,
        max_model_len=2048,
        tensor_parallel_size=1,  # å¼ºåˆ¶ä½¿ç”¨å•GPUï¼Œé¿å…åˆ†å¸ƒå¼é—®é¢˜
        enforce_eager=True,  # ä½¿ç”¨eageræ¨¡å¼ï¼Œæ›´ç¨³å®š
    )
    print("âœ… VLLMæ¨¡å‹åˆå§‹åŒ–æˆåŠŸ")
    
    # è®¾ç½®é‡‡æ ·å‚æ•°
    sampling_params = SamplingParams(
        temperature=0.7,
        top_p=0.9,
        max_tokens=512,
        skip_special_tokens=True
    )
    
    # åŠ è½½AlpacaEvalæ•°æ®é›†
    print("ğŸ“¥ åŠ è½½AlpacaEvalæ•°æ®é›†...")
    dataset = load_dataset("tatsu-lab/alpaca_eval", "alpaca_eval", split="eval")
    
    # å¤„ç†æ ·æœ¬æ•°ï¼š-1è¡¨ç¤ºä½¿ç”¨å…¨éƒ¨æ ·æœ¬
    if max_samples == -1:
        test_set = dataset
        actual_samples = len(dataset)
        print(f"ğŸ“Š ä½¿ç”¨å…¨éƒ¨æ ·æœ¬: {actual_samples}")
    else:
        actual_samples = min(max_samples, len(dataset))
        test_set = dataset.select(range(actual_samples))
        print(f"ğŸ“Š ä½¿ç”¨æ ·æœ¬æ•°: {actual_samples}/{len(dataset)}")
    
    # å‡†å¤‡æ‰€æœ‰è¾“å…¥æç¤º
    print("ğŸ”„ å‡†å¤‡è¾“å…¥æç¤º...")
    prompts = []
    instructions = []
    
    for example in test_set:
        instruction = example['instruction']
        instructions.append(instruction)
        
        # æ„å»ºè¾“å…¥
        messages = [
            {"role": "user", "content": instruction}
        ]
        
        try:
            # åº”ç”¨èŠå¤©æ¨¡æ¿
            text = tokenizer.apply_chat_template(
                messages,
                tokenize=False,
                add_generation_prompt=True
            )
        except:
            # å¦‚æœæ²¡æœ‰èŠå¤©æ¨¡æ¿ï¼Œä½¿ç”¨ç®€å•æ ¼å¼
            text = f"User: {instruction}\nAssistant: "
        
        prompts.append(text)
    
    print(f"ğŸš€ å¼€å§‹æ‰¹é‡ç”Ÿæˆ {len(prompts)} ä¸ªå›å¤...")
    
    # ä½¿ç”¨VLLMæ‰¹é‡ç”Ÿæˆ
    outputs = llm.generate(prompts, sampling_params)
    print("âœ… æ‰¹é‡ç”ŸæˆæˆåŠŸå®Œæˆ")
    
    # æ„å»ºç»“æœ
    results = []
    for i, output in enumerate(outputs):
        response = output.outputs[0].text.strip()
        
        result = {
            "instruction": instructions[i],
            "output": response,
            "generator": model_name
        }
        results.append(result)
        
        print(f"âœ… å®Œæˆæ ·æœ¬ {i+1}/{len(outputs)}: é•¿åº¦ {len(response)}")
    
    print(f"ğŸ‰ æ‰¹é‡ç”Ÿæˆå®Œæˆ! å…±å¤„ç† {len(results)} ä¸ªæ ·æœ¬")
    return results

def run_alpaca_eval_comparison(model1_path, model2_path, output_dir, max_samples=10):
    """
    ä½¿ç”¨å®˜æ–¹AlpacaEvalè¿›è¡Œä¸¤æ¨¡å‹å¯¹æ¯”è¯„ä¼°
    """
    print(f"ğŸš€ å¼€å§‹ä¸¤æ¨¡å‹å¯¹æ¯”è¯„ä¼°")
    print(f"ğŸ“Š æ¨¡å‹1ï¼ˆä¸»è¯„ä¼°ï¼‰: {model1_path}")
    print(f"ğŸ”„ æ¨¡å‹2ï¼ˆå‚è€ƒæ¨¡å‹ï¼‰: {model2_path}")
    
    # éªŒè¯è·¯å¾„
    model1_path = Path(model1_path)
    model2_path = Path(model2_path)
    
    if not model1_path.exists():
        print(f"âŒ æ¨¡å‹1è·¯å¾„ä¸å­˜åœ¨: {model1_path}")
        return False
    
    if not model2_path.exists():
        print(f"âŒ æ¨¡å‹2è·¯å¾„ä¸å­˜åœ¨: {model2_path}")
        return False
    
    # è®¾ç½®è¾“å‡ºç›®å½•
    output_dir = Path(output_dir)
    output_dir.mkdir(exist_ok=True, parents=True)
    
    # è®¾ç½®APIå¯†é’¥ç¯å¢ƒ
    env = os.environ.copy()
    default_api_key = ""
    
    if "OPENAI_API_KEY" not in env:
        env["OPENAI_API_KEY"] = default_api_key
        print(f"âœ… ä½¿ç”¨é»˜è®¤APIå¯†é’¥")
    else:
        print(f"âœ… ä½¿ç”¨ç¯å¢ƒå˜é‡ä¸­çš„APIå¯†é’¥")
    
    try:
        # æ­¥éª¤1: ç”Ÿæˆæ¨¡å‹1çš„è¾“å‡º
        print(f"\nğŸ”„ æ­¥éª¤1: ç”Ÿæˆæ¨¡å‹1è¾“å‡º")
        model1_outputs = generate_model_outputs(str(model1_path), "model1", max_samples)
        
        # æ­¥éª¤2: ç”Ÿæˆæ¨¡å‹2çš„è¾“å‡ºï¼ˆä½œä¸ºå‚è€ƒï¼‰
        print(f"\nğŸ”„ æ­¥éª¤2: ç”Ÿæˆæ¨¡å‹2è¾“å‡ºï¼ˆå‚è€ƒæ¨¡å‹ï¼‰")  
        model2_outputs = generate_model_outputs(str(model2_path), "model2", max_samples)
        
        # ä¿å­˜è¾“å‡ºæ–‡ä»¶
        model1_file = output_dir / "model1_outputs.json"
        model2_file = output_dir / "model2_reference_outputs.json"
        
        with open(model1_file, 'w', encoding='utf-8') as f:
            json.dump(model1_outputs, f, ensure_ascii=False, indent=2)
        
        with open(model2_file, 'w', encoding='utf-8') as f:
            json.dump(model2_outputs, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ’¾ æ¨¡å‹è¾“å‡ºå·²ä¿å­˜:")
        print(f"   ğŸ“ æ¨¡å‹1è¾“å‡º: {model1_file}")
        print(f"   ğŸ“ æ¨¡å‹2å‚è€ƒè¾“å‡º: {model2_file}")
        
        # æ­¥éª¤3: ä½¿ç”¨AlpacaEvalè¿›è¡Œå¯¹æ¯”è¯„ä¼°
        print(f"\nğŸ”„ æ­¥éª¤3: è¿è¡ŒAlpacaEvalå¯¹æ¯”è¯„ä¼°")
        
        # æŒ‰ç…§å®˜æ–¹æ–‡æ¡£ä½¿ç”¨reference_outputså‚æ•°
        cmd = [
            "alpaca_eval", "evaluate",
            "--model_outputs", str(model1_file),
            "--reference_outputs", str(model2_file),
            "--annotators_config", "alpaca_eval_gpt4_turbo_fn",
            "--output_path", str(output_dir)
        ]
        
        print(f"ğŸ”§ æ‰§è¡Œå‘½ä»¤: {' '.join(cmd)}")
        
        # è¿è¡Œè¯„ä¼°å‘½ä»¤
        result = subprocess.run(
            cmd,
            env=env,
            capture_output=True,
            text=True,
            check=True
        )
        
        print(f"âœ… å¯¹æ¯”è¯„ä¼°å®Œæˆ! ç»“æœä¿å­˜åœ¨: {output_dir}")
        print(f"ğŸ“ è¯„ä¼°è¾“å‡º:\n{result.stdout}")
        
        # æŸ¥æ‰¾å¹¶è¯»å–ç»“æœæ–‡ä»¶
        results_files = list(output_dir.glob("**/leaderboard.csv"))
        if results_files:
            print(f"ğŸ“Š ç»“æœæ–‡ä»¶: {results_files[0]}")
            try:
                with open(results_files[0], 'r') as f:
                    content = f.read()
                    print(f"ğŸ“ˆ å¯¹æ¯”è¯„ä¼°ç»“æœ:\n{content}")
            except Exception as e:
                print(f"âš ï¸ æ— æ³•è¯»å–ç»“æœæ–‡ä»¶: {e}")
        
        # åˆ›å»ºè¯„ä¼°æ‘˜è¦
        summary = {
            "model1_path": str(model1_path),
            "model2_path": str(model2_path),
            "evaluation_time": datetime.now().isoformat(),
            "max_samples": max_samples,
            "output_directory": str(output_dir),
            "model1_outputs_file": str(model1_file),
            "model2_reference_file": str(model2_file),
            "command_executed": ' '.join(cmd),
            "status": "success"
        }
        
        summary_file = output_dir / "comparison_summary.json"
        with open(summary_file, 'w', encoding='utf-8') as f:
            json.dump(summary, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ“‹ å¯¹æ¯”æ‘˜è¦ä¿å­˜åœ¨: {summary_file}")
        return True
        
    except subprocess.CalledProcessError as e:
        print(f"âŒ è¯„ä¼°å¤±è´¥:")
        print(f"   é”™è¯¯ä»£ç : {e.returncode}")
        print(f"   é”™è¯¯è¾“å‡º: {e.stderr}")
        print(f"   æ ‡å‡†è¾“å‡º: {e.stdout}")
        return False
    except Exception as e:
        print(f"âŒ æ„å¤–é”™è¯¯: {e}")
        return False

def parse_args():
    parser = argparse.ArgumentParser(description="AlpacaEvalä¸¤æ¨¡å‹å¯¹æ¯”è¯„ä¼°å·¥å…·")
    
    parser.add_argument(
        "--model1_path", 
        type=str, 
        default="/research/projects/trans_llm/Yanshu_Li/conflict/RL/RLresults/Qwen2.5-1.5B-random30/checkpoint-4660",
        help="æ¨¡å‹1è·¯å¾„ï¼ˆä¸»è¯„ä¼°æ¨¡å‹ï¼‰"
    )
    
    parser.add_argument(
        "--model2_path", 
        type=str, 
        default="/research/projects/trans_llm/Yanshu_Li/conflict/RL/RLresults/Qwen2.5-1.5B/checkpoint-15532",
        help="æ¨¡å‹2è·¯å¾„ï¼ˆå‚è€ƒæ¨¡å‹ï¼‰"
    )
    
    parser.add_argument(
        "--output_dir", 
        type=str, 
        default="/research/projects/trans_llm/Yanshu_Li/conflict/RL/alpacaresults/1.5B-random30",
        help="è¯„ä¼°ç»“æœè¾“å‡ºç›®å½•"
    )
    
    parser.add_argument(
        "--max_samples", 
        type=int, 
        default=-1,
        help="æœ€å¤§è¯„ä¼°æ ·æœ¬æ•°ï¼ˆ-1è¡¨ç¤ºä½¿ç”¨å…¨éƒ¨æ ·æœ¬ï¼Œå½“å‰AlpacaEvalçº¦805ä¸ªæ ·æœ¬ï¼‰"
    )
    
    parser.add_argument(
        "--api_key", 
        type=str, 
        default=None,
        help="OpenAI APIå¯†é’¥"
    )
    
    return parser.parse_args()

def main():
    args = parse_args()
    
    print("ğŸ”¬ AlpacaEval ä¸¤æ¨¡å‹å¯¹æ¯”è¯„ä¼°å·¥å…·")
    print("=" * 60)
    
    if args.output_dir is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            args.output_dir = f"alpaca_eval_comparison_{timestamp}"
        
    print(f"ğŸ¥‡ æ¨¡å‹1ï¼ˆä¸»è¯„ä¼°ï¼‰: {args.model1_path}")
    print(f"ğŸ¥ˆ æ¨¡å‹2ï¼ˆå‚è€ƒæ¨¡å‹ï¼‰: {args.model2_path}")
    print(f"ğŸ“Š è¯„ä¼°æ ·æœ¬æ•°: {args.max_samples if args.max_samples != -1 else 'å…¨éƒ¨(~805)'}")
    print(f"ğŸ“ è¾“å‡ºç›®å½•: {args.output_dir}")
        
    success = run_alpaca_eval_comparison(
            model1_path=args.model1_path,
            model2_path=args.model2_path,
            output_dir=args.output_dir,
            max_samples=args.max_samples
        )
        


if __name__ == "__main__":
    main() 